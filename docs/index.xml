<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Jack Bailey</title>
<link>https://www.jack-bailey.co.uk/index.html</link>
<atom:link href="https://www.jack-bailey.co.uk/index.xml" rel="self" type="application/rss+xml"/>
<description>Dr Jack Bailey, Political Scientist</description>
<generator>quarto-1.3.353</generator>
<lastBuildDate>Thu, 03 Aug 2023 23:00:00 GMT</lastBuildDate>
<item>
  <title>Introducing Shannon Regression</title>
  <dc:creator>Jack Bailey</dc:creator>
  <link>https://www.jack-bailey.co.uk/posts/shannon_regression/index.html</link>
  <description><![CDATA[ 




<p>I suspect few political scientists spend time dwelling on information theory. But, over the past year, I’ve spent a lot of time reading and thinking about it. And I am beginning to believe that it has a lot to offer the study of social and political systems.</p>
<p>I’ve found two books on the subject most enlightening. The first is <a href="https://jamesstone.sites.sheffield.ac.uk/books/information-theory">James V. Stone’s “Information Theory: A Tutorial Introduction”</a>. The second is <a href="https://www.simonandschuster.com/books/A-Mind-at-Play/Jimmy-Soni/9781476766690">Jimmy Soni and Rob Goodman’s “A Mind at Play: How Claude Shannon Invented the Information Age”</a>. Together they provide a guide to the basics of information theory and its development as a field.</p>
<p>Information theory owes its existence to one man: Claude Shannon. Shannon more or less invented the field, then solved its most pressing problems, in his landmark paper <a href="https://ieeexplore.ieee.org/document/6773024">“A Mathematical Theory of Communication”</a>. Information theory concerns communications first and foremost. But, really, it’s all about probability. As such, we can apply its insight to any other system that also involves probabilistic outcomes.</p>
<p>In this vain, this blog describes a new type of regression model that I’ve developed. Like logistic or probit regression, it serves to model binary outcomes. But, unlike logistic or probit regression, it uses Shannon’s information content as its link function. This model – which I call “Shannon regression” – has some useful properties. In particular, that it measures coefficients in bits.</p>
<p>I’ll probably write a paper on the method once I understand it better. But I am keen to get it out there so that others can use it and so that I can get some feedback. So, for now, this blog will serve as a kind of way marker. Both for myself, so that I can develop a better understanding of the model, and for others, who might find the project interesting and useful.</p>
<section id="understanding-information-content" class="level2">
<h2 class="anchored" data-anchor-id="understanding-information-content">Understanding Information Content</h2>
<p>To understand Shannon regression, you need to know what information is and how we measure it. If that’s something you know, feel free to skip to the next section. If not, let’s take a moment to spell it all out.</p>
<p>We measure information in “bits”. By definition, 1 bit of information is enough to reduce uncertainty by half. For example, imagine that I toss a fair coin in the air, then prevent you from seeing how it lands. Before you see the coin, you have a 50/50 chance of guessing how it landed. I then reveal that the result is a head. As such, the probabilities change. Because you know how the coin landed, you are certain to guess correctly. You went from choosing between two possible outcomes to one. In other words, your uncertainty halved. Such is the power of 1 bit of information.</p>
<p>To compute the information of some event is straightforward. And this is true no matter whether we’re talking about a coin toss or any other probabilistic event. All we need to do is to use the equation for Shannon’s information content:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(p)%20=%20-%5Cmathrm%7Blog%7D_%7B2%7Dp%0A"></p>
<p>Likewise, we can convert bits of information back into probabilities using the following exponentiation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%202%5E%7B-I%7D%0A"></p>
<p>You might ask “why bother?”. Basically every single advancement in the information age is one reason. Another is that doing yields nice properties that can be useful in certain contexts. Chief among them is that when probabilities are multiplicative, information is additive. For instance, the chance of guessing 1 head is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D">, 2 heads is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20=%20%5Cfrac%7B1%7D%7B4%7D">, and 3 heads is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20=%20%5Cfrac%7B1%7D%7B8%7D">. The amount of information you’d need, on the other hand, is just 1 bit, 2 bits, and 3 bits.</p>
<p>This point is worth stressing: 1 bit of information if how much you’d need to guess a fair coin flip. And since information is additive, it follows that we can interpret it in terms of coin flips too.</p>
<p>I expect most of the people reading this will be political scientists, so let’s use a political example. At the 2019 UK general election, the Labour Party got 32.1% of the vote. So how much information would we need to guess that someone was a Labour voter assuming we knew nothing about them? Well <img src="https://latex.codecogs.com/png.latex?I(0.321)%20%5Capprox%201.64">, so not much at all. It would take more information than we’d need to guess 1 coin flip, but less than to guess 2 in a row. What about one of the smaller parties? In 2019, UKIP got only 0.07% of the vote (yes, really). That gives <img src="https://latex.codecogs.com/png.latex?I(0.0007)%20%5Capprox%2010.48"> bits of information. That’s a lot of information. You’re more or less just as likely to guess that someone you know nothing about voted UKIP as you are to guess 10 coin flips in a row.</p>
</section>
<section id="building-the-model" class="level2">
<h2 class="anchored" data-anchor-id="building-the-model">Building the Model</h2>
<p>Now that you know a little information theory, we can move onto the modelling. All generalised linear models use something called a “link function”. As the name suggests, it is a function that links the outcome scale to some other scale. We do this because it’s often hard to fit a line to the original scale, so we do it on another and transform the result between the two.</p>
<p>Logistic regression, for instance, uses <a href="https://en.wikipedia.org/wiki/Logit">the logit link function</a>. The word “logit” might sound complicated, but it’s really just a fancy way of saying the logarithm of the odds of some probability. Let’s use a fair coin toss again as an example. Since the coin’s fair, you know that the probability, <img src="https://latex.codecogs.com/png.latex?p">, that it lands heads up is 0.5. To convert this probability into logits, we just stick it into the following equation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Blogit%7D(p)%20=%20%5Cmathrm%7Bln%7D%20%5CBigl(%20%5Cfrac%7Bp%7D%7B1%20-%20p%7D%20%5CBigr)%0A"></p>
<p>Here, we first compute the odds of getting a heads, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp%7D%7B1-p%7D%20=%20%5Cfrac%7B0.5%7D%7B1%20-%200.5%7D%20=%201">. Then, we run the resulting odds through the natural logarithm function, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bln%7D">, to convert it to log-odds or logits, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bln%7D(1)%20=%200">. These models also make use of “inverse link functions” that perform the opposite operation. For example, the inverse logit function converts logits back into probabilities. If we have some outcome that we have measured in logits, <img src="https://latex.codecogs.com/png.latex?x">, we can convert it back into probabilities as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-x%7D%7D%0A"></p>
<p>So if we sub in the answer from before we get <img src="https://latex.codecogs.com/png.latex?%5Csigma(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-0%7D%7D%20=%20%5Cfrac%7B1%7D%7B1%20+%201%7D%20=%20%5Cfrac%7B1%7D%7B2%7D%20=%200.5">, the probability of getting a heads on a fair coin.</p>
<p>Other models of binary data use other link functions. The probit model uses the cumulative density function of the normal distribution. Likewise, the cauchit model uses the inverse Cauchy distribution. There is no right or wrong answer here. Different models have different properties that make them more or less useful in certain situations. But most of the time the choice comes down to habit. Economists tend to use probit because they always have done. Other social scientists tend to use logistic regression for the same reason.</p>
<p>In practice, we can use whatever link function we want. As the name suggests, Shannon regression uses the equation for Shannon’s information content as its link function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(p)%20=%20-%5Cmathrm%7Blog%7D_%7B2%7Dp%0A"></p>
<p>And, as its inverse link, it uses the exponentiation from above that turns bits of information back into probabilities:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%202%5E%7B-I%7D%0A"></p>
<p>I thought that implementing this model was going to be really hard. But it turns out that programming custom families in <code>R</code> is really easy. All you have to do is create a function that lays it all out. I’ve called mine “bits” to reflect the unit of measurement:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define "bit" link function</span></span>
<span id="cb1-2"></span>
<span id="cb1-3">bit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb1-4">  <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(){</span>
<span id="cb1-5">    linkfun <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(mu) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(mu, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-6">    linkinv <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(eta) <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^-</span>eta</span>
<span id="cb1-7">    mu.eta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(eta) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span>eta)</span>
<span id="cb1-8">    valideta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(eta) <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">all</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">is.finite</span>(eta) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> eta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) </span>
<span id="cb1-9">    link <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bit"</span></span>
<span id="cb1-10">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">structure</span>(</span>
<span id="cb1-11">      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb1-12">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">linkfun =</span> linkfun,</span>
<span id="cb1-13">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">linkinv =</span> linkinv, </span>
<span id="cb1-14">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mu.eta =</span> mu.eta,</span>
<span id="cb1-15">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">valideta =</span> valideta,</span>
<span id="cb1-16">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">name =</span> link</span>
<span id="cb1-17">      ),</span>
<span id="cb1-18">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link-glm"</span></span>
<span id="cb1-19">    )</span>
<span id="cb1-20">  }</span></code></pre></div>
</div>
<p>Most of this should be pretty self explanatory:</p>
<ul>
<li><p><code>linkfun</code>: Specifies the link function</p></li>
<li><p><code>linkinv</code>: Specifies the inverse link function</p></li>
<li><p><code>mu.eta</code>: Specifies the derivative of the inverse link with respect to eta</p></li>
<li><p><code>valideta</code>: Specifies valid values that eta can take</p></li>
<li><p><code>link</code>: Specifies the name of the custom family</p></li>
<li><p><code>structure</code>: Tells the <code>glm</code> function what everything does</p></li>
</ul>
<p>Let’s simulate some data and run the model. Given the theme of this blog post, we’ll imagine that we’re running an experiment involving coin flips. We recruit <img src="https://latex.codecogs.com/png.latex?n"> participants, then assign them either a 0 or a 1 at random. Those in the control group, where <img src="https://latex.codecogs.com/png.latex?t%20=%200">, get a fair coin where the probability of getting heads is 0.5. Those in the treatment group, where <img src="https://latex.codecogs.com/png.latex?t%20=%201">, get an unfair coin where the probability of getting heads is only 0.25 instead After giving our respondents their coin, we ask them to toss it, then record if they got a heads (1) or a tails (0).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Specify simulation parameters</span></span>
<span id="cb2-2"></span>
<span id="cb2-3">n <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb2-4">fair_heads <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb2-5">unfair_heads <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span></span>
<span id="cb2-6"></span>
<span id="cb2-7"></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Assign respondents to a treatment status</span></span>
<span id="cb2-9"></span>
<span id="cb2-10">treated <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-11">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(</span>
<span id="cb2-12">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-13">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">size =</span> n,</span>
<span id="cb2-14">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">replace =</span> T</span>
<span id="cb2-15">    )</span>
<span id="cb2-16"></span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get coin toss outcomes</span></span>
<span id="cb2-19"></span>
<span id="cb2-20">outcome <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-21">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rbinom</span>(</span>
<span id="cb2-22">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> n,</span>
<span id="cb2-23">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">size =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-24">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prob =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ifelse</span>(treated <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, unfair_heads, fair_heads)</span>
<span id="cb2-25">  )</span>
<span id="cb2-26"></span>
<span id="cb2-27"></span>
<span id="cb2-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fit the model</span></span>
<span id="cb2-29"></span>
<span id="cb2-30">coin_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-31">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glm</span>(</span>
<span id="cb2-32">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">formula =</span> outcome <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> treated,</span>
<span id="cb2-33">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bit</span>())</span>
<span id="cb2-34">  )</span></code></pre></div>
</div>
<p>Now that we’ve fit the model, let’s check its output:</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">&nbsp;(1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: center;">0.971</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.020)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">treated</td>
<td style="text-align: center;">0.992</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.040)</td>
</tr>
</tbody>
</table>


</div>
</div>
<p>Because we used the equation for Shannon’s information content as our link function, the coefficients are measured in bits of information. Like almost any model of a simple experiment, ours has an intercept and a treatment effect. Let’s work out how to interpret them step by step.</p>
<p>The intercept tells us how many bits of information we would need to guess the outcome for someone in the control group. The coefficient itself is 0.97, or about 1 bit of information. This makes sense. Recall that we gave those in the treatment group a fair coin and that we need 1 bit of information to guess the outcome of a fair coin toss.</p>
<p>The treatment effect tells us how many additional bits of information we would need to guess the outcome for someone in the treatment group. The coefficient is 0.99, again about 1 bit of information. When added to the intercept, this gives 1.96, or about 2 bits of information. Again, this makes sense. We gave those in the treatment group an unfair coin that landed heads side up with probability 0.25. So to guess it right, we’d need 2 bits of information, which is how much information it takes to guess the outcome of 2 fair coin flips since <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20=%20%5Cfrac%7B1%7D%7B4%7D">.</p>
<p>At first, it might seem unusual that the treatment caused a <em>negative</em> change in the probability of getting a head but yielded a <em>positive</em> coefficient. But it’s actually quite intuitive when you think about it in information theoretic terms: all we’re doing is counting up coins. And since rarer events are akin to guessing more coin flips correctly, it’s takes <em>more</em> bits of information to guess <em>less</em> frequent events.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>One response to all of this might be: so what? That’s fair enough. I certainly don’t expect people to read this post then switch to using Shannon regression <em>en masse</em>. That said, I do think that there are certain use cases where using Shannon regression might make sense.</p>
<p>To me, the most obvious use case is scenarios which already involve information theoretic quantities. Here, computing effect sizes in bits might prove useful as it would mean that parameter estimates and quantities of interest share a common scale. Likewise, it might be possible to use Shannon regression to decompose sets of outcomes and predictors into known information theoretic quantities.</p>
<p>But, ultimately, I don’t care what the use case is. And I’m sure I have missed some that are blindingly obvious. Shannon regression is neat whatever the case and by putting it out into the world it’ll hopefully find a use in its own time.</p>


</section>

 ]]></description>
  <category>Statistics</category>
  <category>Information Theory</category>
  <guid>https://www.jack-bailey.co.uk/posts/shannon_regression/index.html</guid>
  <pubDate>Thu, 03 Aug 2023 23:00:00 GMT</pubDate>
</item>
</channel>
</rss>
