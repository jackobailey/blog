<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Jack Bailey</title>
<link>https://jack-bailey.co.uk/index.html</link>
<atom:link href="https://jack-bailey.co.uk/index.xml" rel="self" type="application/rss+xml"/>
<description>Dr Jack Bailey, Political Scientist</description>
<generator>quarto-1.3.353</generator>
<lastBuildDate>Mon, 28 Aug 2023 23:00:00 GMT</lastBuildDate>
<item>
  <title>Redshift at the Polls</title>
  <dc:creator>Jack Bailey</dc:creator>
  <link>https://jack-bailey.co.uk/posts/red_shift/index.html</link>
  <description><![CDATA[ 




<p>As you read these words, the heavens are expanding. Soon, distant galaxies will retreat so far away that their light will never reach us again. When this happens, it will be as though they never existed at all. We’ll still be able to see those stars that share our galaxy, but those outside of it will be lost to us forever.</p>
<p>We know this because the light these objects omit is redder than it should be. This occurs because, as the universe expands, it stretches the space between light waves travelling towards us from distant objects. Our eyes then perceive these longer light waves as being more red, leading to a kind of “redshift” in the colour of the stars.</p>
<p>Something similar is happening in British politics. As time passes, historic elections get further and further away from us. While we might still know the key results, the details become more and more uncertain. And, like distant stars in the night’s sky, soon they too will be lost to us forever.</p>
<p>This is a crucial problem for students of British politics. Since the advent of equal suffrage, only 24 general elections have taken place. This is not a large sample. As a result, there is only so much these data can teach us about how British politics works. In an ideal world, the problem would solve itself with the passage of time. With more elections comes more information that we can use to make sense of things. The problem, of course, is that the world is far from ideal.</p>
<p>This is also a crucial problem for British democracy, since the past is a reference against which we orient the present. Two issues spring to mind. First, that electoral uncertainty undermines political legitimacy. If we don’t know what happened, how can we be sure that we arrived where we are in a fair and democratic manner? Second, that the less we know about the past, the worse our ability to make decisions in the present. Are we doomed to vote for the same old mistakes over and over again?</p>
<p>Let’s use the 1945 UK general election as an example. It was, without doubt, the most important in recent British history. If Labour had not won, there would be no NHS and much of the post-war welfare state would look very different. Thus, we would expect to know a lot about it. Some things we know for sure. For instance, the result was a Labour landslide and the election occurred on the 5th July 1945. But others we do not. The official results are, to all intents and purposes, lost to history. We can do our best to piece them together using whatever contemporaneous data we can find, but much uncertainty remains.</p>
<p>When it comes to what happened in 1945, our two best sources are the <a href="https://electiondataarchive.org">Constituency-Level Elections Archive</a> and <a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/S83HOA">Resul Umit’s constituency-level results data</a>. Yet the two don’t even agree on what the vote count was. According to CLEA, it was around 27 million. According to Umit, it was 25 million instead.</p>
<p>Given these differences, we might ask “what do the official statistics say?”. While this is a sensible question, there is one problem: there aren’t any. <a href="https://commonslibrary.parliament.uk/research-briefings/cbp-8647/">The House of Parliament library does host data on elections since 1918</a>. Yet most of these data are second hand and exist only thanks to psephologists like FWS Craig who had the foresight to collect them at the time.</p>
<p>To be clear, I am not looking to deride either CLEA, Umit, or the House of Parliament Library. All do valiant work that we would be much poorer without. Rather, my point is that if we care about British democracy, we need to preserve evidence of it. Otherwise, data will degrade, errors will accumulate, and uncertainty will overwhelm us.</p>
<p>Fundamentally, this is a failing of the British state. Labour’s historic landslide occurred within living memory only 78 years ago. Indeed, there will be thousands of people alive today who were also alive to witness Clement Attlee become Prime Minister. Yet, in this time, we have lost the ability to say what Labour’s vote share was at this most crucial point in its history. Though I have used 1945 as an example here, I’d wager that this is also true for all other intervening elections, since the state has no repository of election results nor has one ever existed.</p>
<p>The question then is what we should do about it. I like FWS Craig’s suggestion: that Returning Officers should have to send data on every election to the Clerk of the Crown.<sup>1</sup> But this alone is not enough. The government should also make the raw data available online in a transparent and open-source format that anyone can access. Further, the data should be permanent, with any changes resulting in a new version of the data and an accompanying change log.</p>
<p>Now is the perfect time to be an astronomer. After all, we live in a time when telescopes exist that are powerful enough to see distant stars, galaxies, and other celestial way markers. That might not always be the case. The same is true for political science: we now know more and have more data than ever before. But if we want that to continue, and to protect our democracy, we need to avoid our own redshift at the polls and do what we can to preserve historic election data.</p>




<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Thanks to <a href="https://twitter.com/EliseUberoi">Elise Uberoi</a> for making me aware of this↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Elections</category>
  <category>Democracy</category>
  <category>Political Science</category>
  <guid>https://jack-bailey.co.uk/posts/red_shift/index.html</guid>
  <pubDate>Mon, 28 Aug 2023 23:00:00 GMT</pubDate>
  <media:content url="https://jack-bailey.co.uk/posts/red_shift/hubble.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Introducing Shannon Regression</title>
  <dc:creator>Jack Bailey</dc:creator>
  <link>https://jack-bailey.co.uk/posts/shannon_regression/index.html</link>
  <description><![CDATA[ 




<p>I doubt many political scientists spend time dwelling on information theory. But, over the past year, I’ve spent a lot of time reading and thinking about it. And I am beginning to believe that it has a lot to offer the study of social and political systems.</p>
<p>I’ve found two books on the subject most enlightening. The first is <a href="https://jamesstone.sites.sheffield.ac.uk/books/information-theory">James V. Stone’s “Information Theory: A Tutorial Introduction”</a>. The second is <a href="https://www.simonandschuster.com/books/A-Mind-at-Play/Jimmy-Soni/9781476766690">Jimmy Soni and Rob Goodman’s “A Mind at Play: How Claude Shannon Invented the Information Age”</a>. Together they provide a guide to the basics and the development of the field.</p>
<p>Information theory owes its existence to one man: Claude Shannon. Shannon more or less invented the field, then solved most of its problems, in his landmark paper <a href="https://ieeexplore.ieee.org/document/6773024">“A Mathematical Theory of Communication”</a>. His idea concerns communications first and foremost. But, really, it’s all about probability. As such, we can apply its insights to any other system that also involves probabilistic outcomes.</p>
<p>In this vein, this post describes a new type of regression model that I’ve developed. Like logistic or probit regression, it models binary outcomes. But, unlike logistic or probit regression, it uses Shannon’s information content as its link function. This model – which I call “Shannon regression” – has some useful properties. In particular, it measures its coefficients in bits.</p>
<p>I’ll probably write a paper on the method once I understand it better. But I am keen to get it out there so that others can use it and so that I can get some feedback. So, for now, this blog will serve as a kind of way marker. Both for myself, so that I can develop a better understanding of the model, and for others, who might find the project interesting and useful.</p>
<section id="understanding-information-content" class="level2">
<h2 class="anchored" data-anchor-id="understanding-information-content">Understanding Information Content</h2>
<p>To understand Shannon regression, you need to know what information is and how to measure it. If that’s something you know, feel free to skip to the next section. If not, let’s take a moment to spell it all out.</p>
<p>We measure information in “bits”. By definition, 1 bit of information reduces our uncertainty by half. For example, imagine that I toss a fair coin in the air, then prevent you from seeing how it lands. Before you see the coin, you have a 50/50 chance of guessing how it landed. I then reveal the result: a head. Now, things change. Because you know how the coin landed, you are certain to guess correctly. You went from choosing between two possible outcomes to one. In other words, your uncertainty halved. Such is the power of 1 bit.</p>
<p>To compute the information of some event is straightforward. This is true whether we’re talking about a coin toss or any other probabilistic event. All we need to do is use the equation for Shannon’s information content:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(p)%20=%20-%5Cmathrm%7Blog%7D_%7B2%7Dp%0A"></p>
<p>Likewise, we can convert bits of information back into probabilities using the following exponentiation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%202%5E%7B-I%7D%0A"></p>
<p>You might ask “why bother?”. Almost every single advance in the information age seems a good enough reason to me. But another is that doing so yields some nice properties that can be useful in certain contexts. Chief among them is that when probabilities multiply, information adds up. For instance, the chance of guessing 1 head is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D">, 2 heads is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20=%20%5Cfrac%7B1%7D%7B4%7D">, and 3 heads is <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20=%20%5Cfrac%7B1%7D%7B8%7D">. But the amount of information you’d need is just 1 bit, 2 bits, and 3 bits.</p>
<p>This point is worth stressing: <em>1 bit of information is how much you’d need to guess a fair coin flip</em>. Since information is additive, it follows that we can interpret it in terms of coin flips too.</p>
<p>Most of the people reading this are likely political scientists, so let’s use a political example. At the 2019 UK general election, the Labour Party got 32.1% of the vote. So how much information would we need to guess that someone was a Labour voter assuming we knew nothing about them? Well <img src="https://latex.codecogs.com/png.latex?I(0.321)%20%5Capprox%201.64">, so not much at all. It would take more information than we’d need to guess 1 coin flip, but less than to guess 2. What about one of the smaller parties? In 2019, UKIP got only 0.07% of the vote (<a href="https://www.bbc.co.uk/news/election/2019/results">yes, really</a>). That gives <img src="https://latex.codecogs.com/png.latex?I(0.0007)%20%5Capprox%2010.48"> bits of information. That’s a lot of information! You have more or less an equal chanceof guessing that someone voted UKIP as you do guessing 10 fair coin flips in a row.</p>
</section>
<section id="building-the-model" class="level2">
<h2 class="anchored" data-anchor-id="building-the-model">Building the Model</h2>
<p>Now that we know a little information theory, we can move onto the modelling. All generalised linear models use something called a “link function”. As the name suggests, it is a function that links the outcome scale to some other scale. We do this because it’s often hard to fit a line to the original scale, so we do it on another one and then transform the resulting predictions back onto the original scale.</p>
<p>Logistic regression, for example, uses <a href="https://en.wikipedia.org/wiki/Logit">the logit link function</a>. The word “logit” might sound complicated, but it’s really just a fancy way of saying that we compute the odds of something happening and then take its logarithm. Let’s use a fair coin toss again as an example. Since the coin’s fair, you know that the probability, <img src="https://latex.codecogs.com/png.latex?p">, of it landing heads up is 0.5. To convert this probability into logits, we just stick it into the following equation:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathrm%7Blogit%7D(p)%20=%20%5Cmathrm%7Bln%7D%20%5CBigl(%20%5Cfrac%7Bp%7D%7B1%20-%20p%7D%20%5CBigr)%0A"></p>
<p>Here, we first compute the odds of getting a heads, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bp%7D%7B1-p%7D%20=%20%5Cfrac%7B0.5%7D%7B1%20-%200.5%7D%20=%201">. Then, we run the resulting odds through the natural logarithm function, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bln%7D">, to convert it to log-odds or logits, <img src="https://latex.codecogs.com/png.latex?%5Cmathrm%7Bln%7D(1)%20=%200">. These models also make use of “inverse link functions” that perform the opposite operation. For example, the inverse logit function converts logits back into probabilities. So if we have some outcome that we have measured in logits, <img src="https://latex.codecogs.com/png.latex?x">, we can convert it back into probabilities as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Csigma(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-x%7D%7D%0A"></p>
<p>And if we sub in the answer from before we get <img src="https://latex.codecogs.com/png.latex?%5Csigma(x)%20=%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-0%7D%7D%20=%20%5Cfrac%7B1%7D%7B1%20+%201%7D%20=%20%5Cfrac%7B1%7D%7B2%7D%20=%200.5">, the probability of getting a heads on a fair coin.</p>
<p>Other models of binary data use other link functions. The probit model uses the cumulative density function of the normal distribution. Likewise, the cauchit model uses the inverse Cauchy distribution. There is no right or wrong answer here. Different models have different properties that make them more or less useful in certain situations. But most of the time the choice comes down to habit. Economists tend to use probit regression because they always have done. Other social scientists tend to use logistic regression for the same reason.</p>
<p>In practice, we can use whatever link function we want. As the name suggests, Shannon regression uses the equation for Shannon’s information content as its link function:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AI(p)%20=%20-%5Cmathrm%7Blog%7D_%7B2%7Dp%0A"></p>
<p>And, as its inverse link, it uses the exponentiation from above that turns bits of information back into probabilities:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%202%5E%7B-I%7D%0A"></p>
<p>I thought that implementing this model was going to be hard. But it turns out that programming custom families in <code>R</code> is really easy. All you have to do is create a function that lays it all out. I’ve called mine “bits” to reflect the unit of measurement:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Define "bit" link function</span></span>
<span id="cb1-2"></span>
<span id="cb1-3">bit <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb1-4">  <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(){</span>
<span id="cb1-5">    linkfun <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(mu) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(mu, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)</span>
<span id="cb1-6">    linkinv <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(eta) <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^-</span>eta</span>
<span id="cb1-7">    mu.eta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(eta) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">log</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">^</span>eta)</span>
<span id="cb1-8">    valideta <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(eta) <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">all</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">is.finite</span>(eta) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&amp;</span> eta <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) </span>
<span id="cb1-9">    link <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"bit"</span></span>
<span id="cb1-10">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">structure</span>(</span>
<span id="cb1-11">      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">list</span>(</span>
<span id="cb1-12">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">linkfun =</span> linkfun,</span>
<span id="cb1-13">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">linkinv =</span> linkinv, </span>
<span id="cb1-14">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">mu.eta =</span> mu.eta,</span>
<span id="cb1-15">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">valideta =</span> valideta,</span>
<span id="cb1-16">        <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">name =</span> link</span>
<span id="cb1-17">      ),</span>
<span id="cb1-18">      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">class =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"link-glm"</span></span>
<span id="cb1-19">    )</span>
<span id="cb1-20">  }</span></code></pre></div>
</div>
<p>Most of this should be pretty self explanatory:</p>
<ul>
<li><p><code>linkfun</code>: Specifies the link function</p></li>
<li><p><code>linkinv</code>: Specifies the inverse link function</p></li>
<li><p><code>mu.eta</code>: Specifies the derivative of the inverse link with respect to eta</p></li>
<li><p><code>valideta</code>: Specifies valid values that eta can take</p></li>
<li><p><code>link</code>: Specifies the name of the custom family</p></li>
<li><p><code>structure</code>: Tells the <code>glm</code> function what everything does</p></li>
</ul>
<p>Let’s simulate some data and run the model. Given the theme of this blog post, we’ll imagine that we’re running an experiment involving coin flips. We recruit <img src="https://latex.codecogs.com/png.latex?n"> participants, then assign them either a 0 or a 1 at random. Those in the control group, where <img src="https://latex.codecogs.com/png.latex?t%20=%200">, get a fair coin where the probability of getting heads is 0.5. Those in the treatment group, where <img src="https://latex.codecogs.com/png.latex?t%20=%201">, get an unfair coin where the probability of getting heads is only 0.25 instead After giving our respondents their coin, we ask them to toss it, then record if they got a heads (1) or a tails (0).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Specify simulation parameters</span></span>
<span id="cb2-2"></span>
<span id="cb2-3">n <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10000</span></span>
<span id="cb2-4">fair_heads <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span></span>
<span id="cb2-5">unfair_heads <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span></span>
<span id="cb2-6"></span>
<span id="cb2-7"></span>
<span id="cb2-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Assign respondents to a treatment status</span></span>
<span id="cb2-9"></span>
<span id="cb2-10">treated <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-11">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sample</span>(</span>
<span id="cb2-12">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-13">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">size =</span> n,</span>
<span id="cb2-14">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">replace =</span> T</span>
<span id="cb2-15">    )</span>
<span id="cb2-16"></span>
<span id="cb2-17"></span>
<span id="cb2-18"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Get coin toss outcomes</span></span>
<span id="cb2-19"></span>
<span id="cb2-20">outcome <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-21">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rbinom</span>(</span>
<span id="cb2-22">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">n =</span> n,</span>
<span id="cb2-23">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">size =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,</span>
<span id="cb2-24">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prob =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">ifelse</span>(treated <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, unfair_heads, fair_heads)</span>
<span id="cb2-25">  )</span>
<span id="cb2-26"></span>
<span id="cb2-27"></span>
<span id="cb2-28"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Fit the model</span></span>
<span id="cb2-29"></span>
<span id="cb2-30">coin_model <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-31">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">glm</span>(</span>
<span id="cb2-32">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">formula =</span> outcome <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> treated,</span>
<span id="cb2-33">    <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">family =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">binomial</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">link =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">bit</span>())</span>
<span id="cb2-34">  )</span></code></pre></div>
</div>
<p>Now that we’ve fit the model, let’s check its output:</p>
<div class="cell">
<div class="cell-output-display">
<table class="table table-sm table-striped small" data-quarto-postprocess="true">
<thead>
<tr class="header">
<th style="text-align: left;" data-quarto-table-cell-role="th"></th>
<th style="text-align: center;" data-quarto-table-cell-role="th">&nbsp;(1)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">(Intercept)</td>
<td style="text-align: center;">0.971</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.020)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">treated</td>
<td style="text-align: center;">0.992</td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: center;">(0.040)</td>
</tr>
</tbody>
</table>


</div>
</div>
<p>Because we used the equation for Shannon’s information content as our link function, the coefficients are measured in bits of information. Like almost any model of a simple experiment, ours has an intercept and a treatment effect. Let’s work out how to interpret them step by step.</p>
<p>The intercept tells us how many bits of information we would need to guess the outcome for someone in the control group. The coefficient itself is 0.971, or about 1 bit of information. This makes sense. Recall that we gave those in the treatment group a fair coin and that we need 1 bit of information to guess the outcome of a fair coin toss.</p>
<p>The treatment effect tells us how many additional bits of information we would need to guess the outcome for someone in the treatment group. The coefficient is 0.992, again about 1 bit of information. When added to the intercept, this gives 1.963, or about 2 bits of information. Again, this makes sense. We gave those in the treatment group an unfair coin that landed heads side up with probability 0.25. So to guess it right, we’d need 2 bits of information, which is how much information it takes to guess the outcome of 2 fair coin flips since <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B2%7D%20%5Ctimes%20%5Cfrac%7B1%7D%7B2%7D%20=%20%5Cfrac%7B1%7D%7B4%7D">.</p>
<p>At first, it might seem unusual that the treatment caused a <em>negative</em> change in the probability of getting a head but yielded a <em>positive</em> coefficient. But it’s actually quite intuitive when you think about it in information theoretic terms: all we’re doing is counting up coins. And since rarer events are akin to guessing more coin flips correctly, it takes <em>more</em> bits of information to guess <em>less</em> frequent events.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I’m prepared to accept that one response to all this might be “so what?”. That’s fair enough. We don’t all learn about information theory and I don’t expect people to read this post then switch to using Shannon regression <em>en masse</em>. That said, I do think that there are certain use cases where Shannon regression might be useful.</p>
<p>The most obvious use case is to use the model to decompose the information content of some event into its constituent causes. For information theoretic studies, this would be especially useful. Another use case is where other parts of the study also use information theoretic quantities. Here, computing effect sizes in bits might would allows all parameter estimates and quantities of interest to share a common scale.</p>
<p>But, ultimately, I don’t care what the use case is. And I’m sure I have missed some that are blindingly obvious. Shannon regression is neat whatever the case and by putting it out into the world it’ll hopefully find a use in its own time.</p>


</section>

 ]]></description>
  <category>Statistics</category>
  <category>Information Theory</category>
  <guid>https://jack-bailey.co.uk/posts/shannon_regression/index.html</guid>
  <pubDate>Thu, 03 Aug 2023 23:00:00 GMT</pubDate>
  <media:content url="https://jack-bailey.co.uk/posts/shannon_regression/shannon.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
